<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>HCI</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="a886148f-3433-4d9e-8c26-1b0eca398d46" class="page sans"><header><img class="page-cover-image" src="https://images.unsplash.com/photo-1516321318423-f06f85e504b3?ixlib=rb-1.2.1&amp;q=85&amp;fm=jpg&amp;crop=entropy&amp;cs=srgb&amp;ixid=eyJhcHBfaWQiOjYzOTIxfQ" style="object-position:center 50%"/><h1 class="page-title">HCI</h1></header><div class="page-body"><figure id="68f429e5-fc43-46c6-9d12-b7aa4b914321" class="link-to-page"><a href="HCI%20a886148f34334d9e8c261b0eca398d46/Observations%20and%20Proposal%2068f429e5fc4346c69d12b7aa4b914321.html"><span class="icon">üëÅÔ∏è‚Äçüó®Ô∏è</span>Observations and Proposal</a></figure><figure id="397c8bbe-7b6d-44f9-96b1-a8ffb4188ac1" class="link-to-page"><a href="HCI%20a886148f34334d9e8c261b0eca398d46/The%20Team%20397c8bbe7b6d44f996b1a8ffb4188ac1.html"><span class="icon">üë•</span>The Team</a></figure><h1 id="9db2290d-54c2-49c6-a1aa-8291a1316823" class="">Project Pitch</h1><h1 id="07806b50-d4b1-49b8-afff-471ade87ce01" class=""><strong>I. </strong><strong>	</strong><strong>The Problem</strong></h1><p id="d2d68430-88c1-4eb7-bac2-deb4083999d6" class="">As the COVID-19 pandemic stretches onward, many of us find ourselves saying when will ‚Äòback to normal‚Äô happen? For most of us, our new realities mean that most, if not all, social interactions, ranging from family reunions to important work-related meetings, take place in the digital world on a video conferencing platform, such as Zoom, Google Meet, and Microsoft Teams. Despite their revolutionizing presence in our society today, these platforms still suffer from issues of connectivity, often leading to instances of unreliable video and audio. Videoconferencing already deals with a loss of human emotion due to a lack of body language, eye contact, clear and in-time facial expressions and voice intonations [1]. Within these limitations, people rely on consistent audio and video quality to properly convey as much emotion as they can for a smooth experience. From the receiving end, users must always be ‚Äòon‚Äô; people must constantly concentrate on their screen in a way that is not required for in-person conversations due to the aforementioned limitations. This has led to a widespread phenomenon that has come to be known as ‚ÄòZoom Fatigue‚Äô [2][3][4][5][7][11]. So the question remains - <strong>how can we convey emotions properly when one of or both audio and video do not allow users to communicate naturally (as they would in person).</strong> Furthermore, could this help alleviate pressure on both sides of the camera to always be ‚Äòon‚Äô?</p><h1 id="70fe186b-1d3c-4fa2-a72d-134a994eea90" class=""><strong>II. </strong><strong>	</strong><strong>The Target Audience / Scope</strong></h1><p id="e5f64f05-eb13-4de2-b871-9e92e410d68e" class="">Not only are moments like this annoying, but for people who are hard of hearing or possess a hearing disability, unreliable video and audio poses an extra challenge. Thus we will concentrate on this problem from the point of view of someone with impaired hearing/someone who relies on the visual aspect of video-conferencing.</p><h1 id="1c2145c9-6e19-4e0c-b3e8-22a3fd57113f" class=""><strong>III. </strong><strong>	</strong><strong>The Solution</strong></h1><p id="9e5a49f9-9221-4811-9d57-b04b8bd60d90" class="">People with difficulties in hearing often rely on movement, facial expressions and lip-reading to understand context and intent. But when that video is removed, this challenging task becomes exponentially harder. Some existing products, such as Skype and Google Meet, already incorporate real-time Speech to text transcription, but only at a rudimentary level. While these products do perform at a basic level and are better than nothing (as is currently the case on Zoom, the primary platform for higher learning), the problem of conveying emotion still exists. With only speech-to-text there is no way of understanding the intent behind the words, we are but actors reading upon a transcript, void of punctuation and descriptions, completely open to interpretation.</p><p id="cc0a017f-d3bd-4be9-8a92-051b40830378" class="">Proposal 1</p><p id="db2780dd-ca2a-480a-916f-65349a64d13b" class="">Our first solution proposes the addition of sentiment analysis of inbound audio in generating captioning. This would allow understanding the communication at a deeper and more human level even with bandwidth that can‚Äôt support high resolution video and audio streaming.</p><p id="fcdf00bb-1976-4d0b-bbe6-e82d1a90045f" class="">Proposal 2</p><p id="ae3e1769-6939-4626-b68f-8ff4e4c548d0" class="">Our second solution proposes, through rudimentary sentiment analysis of inbound audio, to display an avatar reflecting the user‚Äôs current emotion instead of the user&#x27;s icon. Such a solution could be implemented with simple gifs, changing when the emotion changes.</p><p id="be6ac201-7d4b-4adf-a779-6add2327feab" class="">These two possible solutions would work to aid people with hearing difficulties in displaying a visual aid and signifier on the screen to afford emotional interactions and indications. In doing so, we believe this will help to alleviate the pressure users feel to always be ‚Äòon‚Äô and engaged while videoconferencing by mimicking emotions picked up in physical interactions through clear indication on the screen. In addition, the second solution also touches upon the idea of bringing people together through the use of an avatar, rather than static images or text, to represent a person. Finally, these solutions are not exclusive but are intended to be complementary.</p><h1 id="2d4fd614-cff7-4cad-b7f8-973142d04319" class=""><strong>IV. </strong><strong>	</strong><strong>Proposed Methods &amp; Technologies</strong></h1><p id="f9ec2f95-4427-4e3d-94ad-42f3bea6eb05" class="">We plan to scope this project to the English language due to the team members familiarity with this language and the availability of datasets and NLP models for English specifically. After preliminary searches, we have concluded that numerous pre-trained models exist for speech-to-text, notably <a href="https://cloud.google.com/speech-to-text/?gclid=CjwKCAjwnef6BRAgEiwAgv8mQZnZsK9iXDg98GTsGlPVd4sEiIsbql0cLxXRECwBdWtOkKyjNPekoRoCt4QQAvD_BwE">Google Cloud‚Äôs Speech-to-text model</a>. For audio-based sentiment analysis, some preliminary options of base models have been found, but further research is needed to decide on the best course of action.</p><h1 id="2a5241f9-66c2-4eb4-96a0-8239b4d02a22" class=""><strong>V. </strong><strong>	</strong><strong>Propositional Scope &amp; Goals</strong></h1><p id="fd906a8f-0167-4b6a-996b-68abcdb277ac" class="">Proposition 1</p><p id="46e8e19d-6b40-44cb-8771-ffe1d717476e" class="">For this first solution we would consider it complete if the following points are achieved:</p><ul id="4bcdeceb-e6ef-45cb-a814-76159862add7" class="bulleted-list"><li>Basic audio-communication over a local network (since we assume video may not be present);</li></ul><ul id="21cf5fda-fec7-4ad1-bfaf-b7fa78f2e30a" class="bulleted-list"><li>Rudimentary speech to text implementation;</li></ul><ul id="88f3bb2c-8886-4d56-b42f-8d4b67a3c544" class="bulleted-list"><li>2 or more emotions being recognized and displayed as text.</li></ul><p id="111b8768-dbb4-43f8-929c-e35a9a609864" class="">Proposition 2</p><p id="cf499cdc-8ea7-48b2-bef7-40100445c971" class="">For this second solution we would consider it complete if the following points are achieved:</p><ul id="297d0cfc-76e9-434e-81dc-05c48eafbfa6" class="bulleted-list"><li>Basic audio-communication over a local network (we assume that we can generate/display the generated video client-side allowing for less broadband usage);</li></ul><ul id="ed0cec78-58e7-4c54-ba90-20ae28e96174" class="bulleted-list"><li>2 or more emotions being recognized;</li></ul><ul id="f59530ad-5ecf-4797-beff-b698f5569af1" class="bulleted-list"><li>Interchanging of displayed video/animated images depending on the emotion recognized.</li></ul><h1 id="7e76e250-4731-4381-b7dd-089e313057fe" class=""><strong>Resources:</strong></h1><p id="3293764c-b568-4240-96fe-1fc9d3c3c803" class="">[1]	W. Buxton, ‚ÄúTelepresence: integrating shared task and person spaces.‚Äù 1992. <em>Proceedings of Graphics Interface &#x27;92, </em>123-129. Earlier version appears in <em>Proceedings of Groupware &#x27;91</em>, Amsterdam, Oct. 29, 1991, 27-36.</p><p id="90505b1a-fd0d-40a2-8c54-dfa2ecdc339e" class="">[2]	P. deHahn, ‚ÄúZoom fatigue is something the deaf community knows very well,‚Äù <em>Quartz</em>, 14-May-2020. [Online]. Available: https://qz.com/1855404/zoom-fatigue-is-something-the-deaf-community-knows-very-well/. [Accessed: 11-Sep-2020].</p><p id="3e7c60e3-dc43-4b16-ab57-eabe3fb730a8" class="">[3]	R. El Kaliouby, ‚ÄúEmotionally aware technology could help us beat Zoom fatigue,‚Äù <em>Fast Company</em>, 12-Jun-2020. [Online]. Available: https://www.fastcompany.com/90515714/emotionally-aware-technology-could-help-us-beat-zoom-fatigue. [Accessed: 11-Sep-2020].</p><p id="6bcefd18-cdca-4a55-90c9-a90f5bdd4e28" class="">[4]	M. Jiang, ‚ÄúThe reason Zoom calls drain your energy,‚Äù <em>BBC Worklife</em>, 22-Apr-2020. [Online]. Available: https://www.bbc.com/worklife/article/20200421-why-zoom-video-chats-are-so-exhausting. [Accessed: 11-Sep-2020].</p><p id="eaa2c0b9-8756-4320-bf2d-f6c022a2d1d6" class="">[5]	L. Kaushik, A. Sangwan, and J. H. L. Hansen, ‚ÄúSentiment extraction from natural audio streams,‚Äù <em>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 2013.</p><p id="eb3ca2c7-b560-4f8a-81aa-2140fa3e50c4" class="">[6]	I. Noon, ‚ÄúBeing deaf and dealing with concentration fatigue,‚Äù <em>Making deaf children matter</em>, 29-Jun-2013. [Online]. Available: https://iannoon.wordpress.com/2013/06/27/being-deaf-and-knackered/. [Accessed: 11-Sep-2020].</p><p id="dcacdf41-b128-4583-b34a-9bd780dcc7e0" class="">[7]	P. by B. Rasmussen, ‚Äú&#x27;Zoom fatigue&#x27; is taxing the brain. Here&#x27;s why that happens.,‚Äù <em>National Geographic</em>, 24-Apr-2020. [Online]. Available: https://www.nationalgeographic.com/science/2020/04/coronavirus-zoom-fatigue-is-taxing-the-brain-here-is-why-that-happens/. [Accessed: 11-Sep-2020].</p><p id="71cc7454-1c77-43b2-892b-f8cc9bc17431" class="">[8]	S. Seo, S. Na, and J. Kim, ‚ÄúHMTL: Heterogeneous Modality Transfer Learning for Audio-Visual Sentiment Analysis,‚Äù <em>IEEE Access</em>, vol. 8, pp. 140426‚Äì140437, 2020.</p><p id="2dbd51c3-6431-4947-b0d7-584221a7f7b6" class="">[9]	C. Stern, ‚ÄúHear and Connect: Zoom and Captioning for Hearing Loss: CHC blog,‚Äù <em>Center for Hearing and Communication</em>, 02-Apr-2020. [Online]. Available: https://chchearing.org/blog/zoom-captioning-hearing-loss/. [Accessed: 11-Sep-2020].</p><p id="e8fcc0e0-be57-4c24-a611-f74482ca9632" class="">[10]	C. Vogler, ‚ÄúAccessibility Tips for a Better Zoom/Virtual Meeting Experience,‚Äù <em>Deaf/Hard of Hearing Technology Rehabilitation Engineering Research Center</em>, 21-May-2020. [Online]. Available: https://www.deafhhtech.org/rerc/accessible-virtual-meeting-tips/. [Accessed: 11-Sep-2020].</p><p id="6fa564b7-2653-486d-9bdc-9b2f9775bbf3" class="">[11]	‚ÄúWhat is Concentration Fatigue?,‚Äù <em>Family Audiology</em>, 12-Dec-2019. [Online]. Available: http://familyaudiologyonline.com/what-is-concentration-fatigue/. [Accessed: 11-Sep-2020].</p></div></article></body></html>